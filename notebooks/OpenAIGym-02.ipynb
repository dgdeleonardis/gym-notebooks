{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "702c6d47",
   "metadata": {},
   "source": [
    "## Introduzione\n",
    "Il seguente notebook è frutto della visione del video [Play Any OpenAI Gym Environment with a Single Agent](https://www.youtube.com/watch?v=nvhWfk7R0RM&list=PLIfPjWrv526bMF8_vx9BqWjec-F-g-lQO&index=2) di TheComputerScientist \n",
    "\n",
    "In questo notebook si studiano altri due environment: **MountainCarContinuous-v0** e **MountainCar-v0** (*versione discreta*)\n",
    "\n",
    "Nota bene: quando si parla di environment discreto o continuo si fa riferimento in questo caso all'**action space**.\n",
    "\n",
    "***\n",
    "\n",
    "## Import\n",
    "\n",
    "Per prima cosa si fa il setup dell'ambiente di lavoro, importando la libreria:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55552a10",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d870ae1",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Creazione dell'environment **MountainCar-v0** e di un agente stupido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66f499f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/diegodeleonardis/Documents/università/tirocinio/open_ia_gym_env/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:97: UserWarning: \u001b[33mWARN: We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "env_name = \"MountainCar-v0\"\n",
    "env = gym.make(env_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03aaaddc",
   "metadata": {},
   "source": [
    "E si stampano l'**observation space** e l'**action space**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4fe4ab51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation space of MountainCar-v0 environment: Box([-1.2  -0.07], [0.6  0.07], (2,), float32)\n",
      "Action space of MountainCar-v0 environment: Discrete(3)\n"
     ]
    }
   ],
   "source": [
    "print(\"Observation space of\", env_name, \"environment:\", env.observation_space)\n",
    "print(\"Action space of\", env_name, \"environment:\", env.action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf796449",
   "metadata": {},
   "source": [
    "Dopodichè si definisce la classe Agent che andrà ad interagire con l'environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "556fb378",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, env):\n",
    "        self.action_size = env.action_space.n\n",
    "        print(\"Action size:\", self.action_size)\n",
    "        \n",
    "    def get_action(self, state):\n",
    "        action = random.choice(range(self.action_size))\n",
    "        return action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8937266f",
   "metadata": {},
   "source": [
    "E successivamente si istanzia un agent e facendogli eseguire delle azioni casuali come test sull'environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d4f04d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action size: 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.50772446,  0.        ], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent = Agent(env)\n",
    "state = env.reset()\n",
    "\n",
    "for _ in range(200):\n",
    "    action = agent.get_action(state)\n",
    "    state, reward, done, info = env.step(action)\n",
    "    env.render()\n",
    "    \n",
    "env.close()\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8daee7",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Creazione dell'environment **MountainCarContinuos-v0** e di un agente stupido\n",
    "\n",
    "Si impiega ora l'environment continuo **MountainCarContinuous-v0**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97bc8573",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name_continuous = \"MountainCarContinuous-v0\"\n",
    "env_continuous = gym.make(env_name_continuous)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "868c6871",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation space of MountainCarContinuous-v0 environment: Box([-1.2  -0.07], [0.6  0.07], (2,), float32)\n",
      "Action space of MountainCarContinuous-v0 environment: Box(-1.0, 1.0, (1,), float32)\n"
     ]
    }
   ],
   "source": [
    "print(\"Observation space of\", env_name_continuous, \"environment:\", env_continuous.observation_space)\n",
    "print(\"Action space of\", env_name_continuous, \"environment:\", env_continuous.action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf287270",
   "metadata": {},
   "source": [
    "Come si può notare in questo caso si ha un ``action_space`` continuo di tipo ```float32```.\n",
    "\n",
    "La classe **Agent** definita in precedenza non è valida per environment continui e ciò è dato dal fatto che ``action_space`` non ha una dimensione n e quindi l'azione casuale non è ottenibile tramite il ``random.choice()`` (*returns a randomly selected element from the specified sequence*). In questo caso si utilizza la libreria **numpy** per definire una nuova classe agente valida per environment continui:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a04003a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentCont:\n",
    "    def __init__(self, env):\n",
    "        self.action_low = env.action_space.low\n",
    "        self.action_high = env.action_space.high\n",
    "        self.action_shape = env.action_space.shape\n",
    "        print(self.action_shape)\n",
    "        \n",
    "    def get_action(self, state):\n",
    "        action = np.random.uniform(self.action_low,\n",
    "                                  self.action_high,\n",
    "                                  self.action_shape)\n",
    "        return action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f839a1",
   "metadata": {},
   "source": [
    "Il passo successivo consiste nel far interagire questo nuovo tipo di agente con l'environment **MountainCarContinuous-v0**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "71fc1f73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.42414397,  0.        ], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_cont = AgentCont(env_continuous)\n",
    "state = env_continuous.reset()\n",
    "\n",
    "for _ in range(200):\n",
    "    action = agent_cont.get_action(state)\n",
    "    state, reward, done, info = env_continuous.step(action)\n",
    "    env_continuous.render()\n",
    "    \n",
    "env_continuous.close()\n",
    "env_continuous.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41271c1a",
   "metadata": {},
   "source": [
    "Per curiosità si prova ad utilizzare la classe **Agent** per verificare quanto detto in precedenza:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ecff3e7c",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Box' object has no attribute 'n'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m agent \u001b[38;5;241m=\u001b[39m \u001b[43mAgent\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv_continuous\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m state \u001b[38;5;241m=\u001b[39m env_continuous\u001b[38;5;241m.\u001b[39mreset()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m200\u001b[39m):\n",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36mAgent.__init__\u001b[0;34m(self, env)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, env):\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_size \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maction_space\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn\u001b[49m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAction size:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_size)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Box' object has no attribute 'n'"
     ]
    }
   ],
   "source": [
    "agent = Agent(env_continuous)\n",
    "state = env_continuous.reset()\n",
    "\n",
    "for _ in range(200):\n",
    "    action = agent.get_action(state)\n",
    "    state, reward, done, info = env_continuous.step(action)\n",
    "    env_continuous.render()\n",
    "    \n",
    "env_continuous.close()\n",
    "env_continuous.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f513788",
   "metadata": {},
   "source": [
    " ***\n",
    " \n",
    " ## Creazione di un nuovo agente\n",
    " \n",
    " Si determina ora una classe **Agent** in grado di interagire con environment discreti e continui:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7245b931",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentV2:\n",
    "    def __init__(self, env):\n",
    "        self.is_discrete = type(env.action_space) == gym.spaces.discrete.Discrete\n",
    "        \n",
    "        if self.is_discrete:\n",
    "            self.action_size = env.action_space.n\n",
    "            print(\"Action size:\", self.action_size)\n",
    "        else:\n",
    "            self.action_low = env.action_space.low\n",
    "            self.action_high = env.action_space.high\n",
    "            self.action_shape = env.action_space.shape\n",
    "            print(\"Action range:[\", self.action_low,\",\", self.action_high, \"]\")\n",
    "            \n",
    "    def get_action(self, state):\n",
    "        if self.is_discrete:\n",
    "            action = random.choice(range(self.action_size))\n",
    "        else:\n",
    "            action = np.random.uniform(self.action_low,\n",
    "                                      self.action_high,\n",
    "                                      self.action_shape)\n",
    "\n",
    "        return action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734ffb45",
   "metadata": {},
   "source": [
    "E si prova a verificare il funzionamento di ```AgentV2``` sui due nuovi environment, uno discreto (**Acrobot-v1**) e uno continuo (**Pendulum-v1**):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "69b576f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action size: 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.9951293 , -0.09857865,  0.99986255,  0.01658019,  0.08299524,\n",
       "       -0.06223753], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env1_name = \"Acrobot-v1\"\n",
    "env1 = gym.make(env1_name)\n",
    "\n",
    "agent = AgentV2(env1)\n",
    "state = env1.reset()\n",
    "\n",
    "for _ in range(200):\n",
    "    action = agent.get_action(state)\n",
    "    state, reward, done, info = env1.step(action)\n",
    "    env1.render()\n",
    "    \n",
    "env1.close()\n",
    "env1.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "53824619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action range:[ [-2.] , [2.] ]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.1871025 , -0.9823404 ,  0.16972768], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env2_name = \"Pendulum-v1\"\n",
    "env2 = gym.make(env2_name)\n",
    "\n",
    "agent = AgentV2(env2)\n",
    "state = env2.reset()\n",
    "\n",
    "for _ in range(200):\n",
    "    action = agent.get_action(state)\n",
    "    state, reward, done, info = env2.step(action)\n",
    "    env2.render()\n",
    "    \n",
    "env2.close()\n",
    "env2.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94755433",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
